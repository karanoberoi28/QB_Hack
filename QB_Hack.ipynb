{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/QB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACRTA road taxation data engineering challange by Quantumblack\n",
    "Contest Link: https://datahack.analyticsvidhya.com/contest/quantumblack-online-hackathon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACRTA road taxation data engineering\n",
    "\n",
    "Astro City Road transport authority (ACRTA) in US have come up with an idea to use car registration renewal charges to provide indirect incentives to safe drivers. Also, providing subsidies to certain areas as per the extreme climatic conditions in terms of heavy snow or rain.\n",
    "\n",
    "ACRTA has contacted us to perform a quantitative study and design a prediction model to support the aforementioned applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/QB1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, being a part of the data engineering team, are working continuously with the business stakeholders as well as data scientists to create features around these scenarios.\n",
    "\n",
    "Problem statement that we have been provided is to “Develop inputs for a model that predicts the chances of having a vehicle accident based on driving conditions. This model will help the transport authority to understand risk patterns and act upon them.”\n",
    "\n",
    "This output then would be utilized so as to come up for a risk-based taxation on different drivers and locations as per crash-prone weather conditions.\n",
    "\n",
    "Use cases would be –\n",
    "\n",
    "#### Imposing “unsafe driving tax” on drivers to provide a positive feedback loop which may be revisited every year by looking at the past year trip data based on the driving patterns. Lower the tax in the regions where the climatic conditions lead them to become a crash-prone site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Drive Data (Connected car data) – Data coming from the car-mounted devices, which provides you with the car statistics every second. This information will include – Speed, acceleration, engine temperature and other car statistics.\n",
    "\n",
    "#### 2. Trip – Parameters associated with location of car such as lattitude, longitude, altitude and other similar parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/trip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Weather – Weather condition at different latitude & longitude during different times each day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/weather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Vehicle Specifications – Different vehicle technical specifications which comes from the manufacturer of the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission/Output Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input data as described above, the participant would need to create features related to different hypothesis around the driving behaviours.\n",
    "\n",
    "Solution file must be a single zip containing the 3 csvs for 3 kinds of features with the file names as given below:\n",
    "\n",
    "#### Engine Features: engine_features.csv Drive Features: drive_features.csv Weather Features: weather_features.csv The csv files must have the same file names and columns in order so that checks may be done. Failure to do so would result in direct rejection as these would be tested through automated scripts.\n",
    "\n",
    "To understand the type of features to create, Please go through the requirements for the features carefully:\n",
    "#### 1. Engine Features (file name – engine_features.csv) \n",
    "Grain - every vehicle aggregated at week start date(Monday) for the complete week in YYYY-MM-DD format.\n",
    "\n",
    "Sorted - by Vehicle ID and week_start_Date in ascending manner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/engine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "Convert timezone to PST before any calculations All vehicles from drive data should be in the final output even if you do not have specifications (Fill with 0 if specs are not given) \n",
    "Active horsepower - Engine load / 255 Max Torque RPM / 5252 \n",
    "Horsepower utilization – Active horsepower / Max Horsepower Torque Utilization - calculated as Engine load/ 255 \n",
    "RPM Utilization – RPM / Maximum horsepower rpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Drive features(file name – drive_features.csv) \n",
    "\n",
    "Grain – Every trip’s aggregated features at a trip id level.\n",
    "\n",
    "Sorted - by trip_id in ascending manner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/drive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "Acceleration m/s is calculated as a change in velocity over time If a vehicle keeps on accelerating continuously over a period of time, please treat them as a single acceleration or deacceleration period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Weather features (file name – weather_features.csv) \n",
    "\n",
    "Here are the weather conditions for your reference and generating weather feature accordingly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/weather1.png)\n",
    "![](img/weather2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grain – Every vehicle detail should be aggregated at a week start date.\n",
    "\n",
    "Sorted - by vehicle_id and week_start_date in ascending manner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/weath.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: convert time zone to PST before any calculations\n",
    "\n",
    "Assumptions & Hints–\n",
    "\n",
    "Weather data is already in PST and may not need any timezone conversion. You may consider the weather data to be constant for complete hour basis. For example- if the temperature is given to be 284.51 for 2017-02-14 19:00:00, it would be the same for time 2017-02-14 19:15:45 as well. Haversine formula must be utilized to measure the distance between any 2 consecutive points in between the trips. Matches in between datasets must be on geohash precision point 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/hav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studies have found that - The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Important in navigation, it is a special case of a more general formula in spherical trigonometry, the law of haversines, that relates the sides and angles of spherical triangles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import parquet\n",
    "import pytz\n",
    "from datetime import datetime,timedelta\n",
    "from pytz import timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pygeohash as gh\n",
    "import warnings\n",
    "from haversine import haversine, Unit\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "owd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './input_data/weather'\n",
    "path2 = './input_data/trip'\n",
    "path3 = './input_data/drive'\n",
    "\n",
    "def read_data(file_path):\n",
    "    \n",
    "    os.chdir (file_path)\n",
    "    parquet_list=list()\n",
    "    for filename in glob.glob('*.parquet'):\n",
    "        parquet_list.append(filename)\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for m in range(0,len(parquet_list)):\n",
    "        temp = pd.read_parquet(parquet_list[m],engine='fastparquet')\n",
    "        df = df.append(temp)\n",
    "    \n",
    "    del temp\n",
    "    return df\n",
    "\n",
    "weather_data = read_data(path1);os.chdir(owd)\n",
    "trip_data = read_data(path2);os.chdir(owd)\n",
    "drive_data = read_data(path3);os.chdir(owd)\n",
    "vehicle_data = pd.read_csv('./input_data/vehicle.csv', low_memory=False);os.chdir(owd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Trip and Vehicle Data to Drive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./input_data')\n",
    "\n",
    "drive_data['datetime'] = drive_data['datetime'].dt.tz_localize('US/Pacific')\n",
    "trip_data['datetime'] = trip_data['datetime'].dt.tz_localize('US/Pacific')\n",
    "trip_data['geohash'] = trip_data.apply(lambda x: gh.encode(x.lat, x.long, precision = 5),axis = 1)\n",
    "\n",
    "drive_data = pd.merge(drive_data, trip_data, how='left', on=['vehicle_id','trip_id','datetime'])\n",
    "#plt.plot(drive_data['velocity_x'],drive_data['velocity_y'])\n",
    "drive_data['velocity'] = drive_data['velocity_x']\n",
    "drive_data = drive_data.drop(columns=['velocity_x', 'velocity_y'])\n",
    "\n",
    "drive_data = drive_data.sort_values(['vehicle_id','trip_id','datetime'], ascending=[True, True, True])    \n",
    "drive_data['date'] = pd.to_datetime(drive_data['datetime'].astype(str).str[0:10],format = '%Y-%m-%d')\n",
    "\n",
    "#Merging Vehicle Specification\n",
    "drive_data = pd.merge(drive_data, vehicle_data, how='left', on=['vehicle_id'])\n",
    "drive_data.reset_index(level=0, inplace=True)\n",
    "\n",
    "#checking for NULL\n",
    "#drive_data.isnull().sum(axis = 0)\n",
    "\n",
    "drive_data = drive_data.drop(columns=['accel_x','accel_y','accel_z','iat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Engine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ft_torque_util_60pct_s</th>\n",
       "      <th>ft_torque_util_70pct_s</th>\n",
       "      <th>ft_torque_util_80pct_s</th>\n",
       "      <th>ft_torque_util_90pct_s</th>\n",
       "      <th>ft_horsepower_util_50pct_s</th>\n",
       "      <th>ft_horsepower_util_60pct_s</th>\n",
       "      <th>ft_horsepower_util_70pct_s</th>\n",
       "      <th>ft_horsepower_util_80pct_s</th>\n",
       "      <th>ft_rpm_util_50pct_s</th>\n",
       "      <th>ft_rpm_util_60pct_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>2692</td>\n",
       "      <td>1014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>227</td>\n",
       "      <td>124005</td>\n",
       "      <td>87362</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>37</td>\n",
       "      <td>7214</td>\n",
       "      <td>9932</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>41</td>\n",
       "      <td>37783</td>\n",
       "      <td>19878</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>14723</td>\n",
       "      <td>5937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id week_start_date  ft_torque_util_60pct_s  ft_torque_util_70pct_s  \\\n",
       "0     1000500      2016-12-26                       0                    2692   \n",
       "1     1000500      2017-01-02                     227                  124005   \n",
       "2     1000500      2017-01-09                      37                    7214   \n",
       "3     1000500      2017-01-16                      41                   37783   \n",
       "4     1000500      2017-01-23                       0                   14723   \n",
       "\n",
       "   ft_torque_util_80pct_s  ft_torque_util_90pct_s  ft_horsepower_util_50pct_s  \\\n",
       "0                    1014                       0                           0   \n",
       "1                   87362                      85                           0   \n",
       "2                    9932                     122                           0   \n",
       "3                   19878                     116                           0   \n",
       "4                    5937                       0                           0   \n",
       "\n",
       "   ft_horsepower_util_60pct_s  ft_horsepower_util_70pct_s  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   ft_horsepower_util_80pct_s  ft_rpm_util_50pct_s  ft_rpm_util_60pct_s  \n",
       "0                           0                    0                    0  \n",
       "1                           0                    0                    0  \n",
       "2                           0                    0                    0  \n",
       "3                           0                    0                    0  \n",
       "4                           0                    0                    0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_data['torque_util'] = drive_data['eng_load'].astype(float) / 255.0\n",
    "drive_data['rpm_util'] = drive_data['rpm'].astype(float) / drive_data['max_horsepower_rpm'].astype(float)\n",
    "\n",
    "drive_data['act_hp'] = (drive_data['torque_util'].astype(float) * drive_data['max_torque'].astype(float) * drive_data['rpm'].astype(float))\n",
    "drive_data['act_hp'] = drive_data['act_hp'].astype(float) / 5252.0\n",
    "drive_data['hp_util'] = drive_data['act_hp'].astype(float) / drive_data['max_horsepower'].astype(float)\n",
    "\n",
    "#Extracting first day of the week\n",
    "drive_data[\"First_day_of_the_week\"] = drive_data['date'].apply(lambda x: (x - timedelta(days=x.dayofweek)))\n",
    "\n",
    "#Creating Row Counter\n",
    "drive_data['row_temp']=1\n",
    "drive_data['row_count'] = drive_data.groupby(['trip_id'])['row_temp'].apply(lambda x: x.cumsum()) \n",
    "\n",
    "#Calculating Utilization features\n",
    "\n",
    "drive_data['ft_torque_util_60pct_s'] = 0\n",
    "drive_data['ft_torque_util_60pct_s'][(drive_data['torque_util']>=0.6) & (drive_data['torque_util']<0.7)]=1\n",
    "drive_data['ft_torque_util_70pct_s'] = 0\n",
    "drive_data['ft_torque_util_70pct_s'][(drive_data['torque_util']>=0.7) & (drive_data['torque_util']<0.8)]=1\n",
    "drive_data['ft_torque_util_80pct_s'] = 0\n",
    "drive_data['ft_torque_util_80pct_s'][(drive_data['torque_util']>=0.8) & (drive_data['torque_util']<0.9)]=1\n",
    "drive_data['ft_torque_util_90pct_s'] = 0\n",
    "drive_data['ft_torque_util_90pct_s'][(drive_data['torque_util']>=0.9) & (drive_data['torque_util']<1.0)]=1\n",
    "\n",
    "drive_data['ft_horsepower_util_50pct_s'] = 0\n",
    "drive_data['ft_horsepower_util_50pct_s'][(drive_data['hp_util']>=0.5) & (drive_data['hp_util']<0.6)]=1\n",
    "drive_data['ft_horsepower_util_60pct_s'] = 0\n",
    "drive_data['ft_horsepower_util_60pct_s'][(drive_data['hp_util']>=0.6) & (drive_data['hp_util']<0.7)]=1\n",
    "drive_data['ft_horsepower_util_70pct_s'] = 0\n",
    "drive_data['ft_horsepower_util_70pct_s'][(drive_data['hp_util']>=0.7) & (drive_data['hp_util']<0.8)]=1\n",
    "drive_data['ft_horsepower_util_80pct_s'] = 0\n",
    "drive_data['ft_horsepower_util_80pct_s'][(drive_data['hp_util']>=0.8) & (drive_data['hp_util']<0.9)]=1\n",
    "\n",
    "drive_data['ft_rpm_util_50pct_s'] = 0\n",
    "drive_data['ft_rpm_util_50pct_s'][(drive_data['rpm_util']>=0.5) & (drive_data['rpm_util']<0.6)]=1\n",
    "drive_data['ft_rpm_util_60pct_s'] = 0\n",
    "drive_data['ft_rpm_util_60pct_s'][(drive_data['rpm_util']>=0.6) & (drive_data['rpm_util']<0.7)]=1\n",
    "\n",
    "\n",
    "drive_data_req = drive_data[['vehicle_id','First_day_of_the_week','ft_torque_util_60pct_s','ft_torque_util_70pct_s','ft_torque_util_80pct_s','ft_torque_util_90pct_s','ft_horsepower_util_50pct_s','ft_horsepower_util_60pct_s','ft_horsepower_util_70pct_s','ft_horsepower_util_80pct_s','ft_rpm_util_50pct_s','ft_rpm_util_60pct_s']]\t\n",
    "engine_features = drive_data_req.groupby(['vehicle_id','First_day_of_the_week'], as_index=False).sum()\n",
    "engine_features=engine_features.rename(columns = {'First_day_of_the_week':'week_start_date'})\n",
    "del drive_data_req\n",
    "engine_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_data = drive_data.drop(columns=['drivetrain','max_torque','max_horsepower','max_horsepower_rpm','max_torque_rpm','engine_displacement','fuel_type'])\n",
    "drive_data = drive_data.drop(columns=['fuel_tank_capacity','fuel_economy_city','fuel_economy_highway','cylinders','forced_induction','device_generation','rpm_util','act_hp','hp_util'])\n",
    "drive_data = drive_data.drop(columns=['ft_torque_util_60pct_s','ft_torque_util_70pct_s','ft_torque_util_80pct_s','ft_torque_util_90pct_s','ft_horsepower_util_50pct_s','ft_horsepower_util_60pct_s','ft_horsepower_util_70pct_s','ft_horsepower_util_80pct_s','ft_rpm_util_50pct_s','ft_rpm_util_60pct_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Drive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>ft_sum_hard_brakes_10_flg_val</th>\n",
       "      <th>ft_sum_hard_brakes_3_flg_val</th>\n",
       "      <th>ft_sum_hard_accel_10_flg_val</th>\n",
       "      <th>ft_sum_hard_accel_3_flg_val</th>\n",
       "      <th>ft_sum_time_deaccel_val</th>\n",
       "      <th>ft_sum_time_accel_val</th>\n",
       "      <th>ft_cnt_vehicle_deaccel_val</th>\n",
       "      <th>ft_cnt_vehicle_accel_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00922df3be5a4589ab385d0c2da2dd81</td>\n",
       "      <td>15</td>\n",
       "      <td>536</td>\n",
       "      <td>8</td>\n",
       "      <td>515</td>\n",
       "      <td>1414.0</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00dc31fe55e24d14989c89de4b3b683b</td>\n",
       "      <td>52</td>\n",
       "      <td>844</td>\n",
       "      <td>82</td>\n",
       "      <td>801</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>1217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0156d21e316d4d8b9d5bf6ccff797bf7</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>1135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01b8a24510cd4e4684d67b96369286e0</td>\n",
       "      <td>12</td>\n",
       "      <td>139</td>\n",
       "      <td>10</td>\n",
       "      <td>141</td>\n",
       "      <td>312.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01c2a70c25e5428bb33811ca5eb19270</td>\n",
       "      <td>1</td>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>682</td>\n",
       "      <td>3518.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>2374.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            trip_id  ft_sum_hard_brakes_10_flg_val  \\\n",
       "0  00922df3be5a4589ab385d0c2da2dd81                             15   \n",
       "1  00dc31fe55e24d14989c89de4b3b683b                             52   \n",
       "2  0156d21e316d4d8b9d5bf6ccff797bf7                              1   \n",
       "3  01b8a24510cd4e4684d67b96369286e0                             12   \n",
       "4  01c2a70c25e5428bb33811ca5eb19270                              1   \n",
       "\n",
       "   ft_sum_hard_brakes_3_flg_val  ft_sum_hard_accel_10_flg_val  \\\n",
       "0                           536                             8   \n",
       "1                           844                            82   \n",
       "2                           218                             1   \n",
       "3                           139                            10   \n",
       "4                           689                             1   \n",
       "\n",
       "   ft_sum_hard_accel_3_flg_val  ft_sum_time_deaccel_val  \\\n",
       "0                          515                   1414.0   \n",
       "1                          801                   1853.0   \n",
       "2                          230                   1746.0   \n",
       "3                          141                    312.0   \n",
       "4                          682                   3518.0   \n",
       "\n",
       "   ft_sum_time_accel_val  ft_cnt_vehicle_deaccel_val  ft_cnt_vehicle_accel_val  \n",
       "0                 1482.0                       976.0                     974.0  \n",
       "1                 1835.0                      1217.0                    1217.0  \n",
       "2                 1686.0                      1134.0                    1135.0  \n",
       "3                  317.0                       210.0                     210.0  \n",
       "4                 3613.0                      2375.0                    2374.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Time deltas\n",
    "drive_data['tdelta'] = (drive_data['datetime']-drive_data['datetime'].shift()).fillna(0)\n",
    "drive_data['ans'] = drive_data['tdelta'].dt.total_seconds()\n",
    "drive_data['ans'][drive_data['row_count']==1]=0.0\n",
    "drive_data['tdelta'] = drive_data['ans']\n",
    "drive_data = drive_data.drop(columns=['ans'])\n",
    "#plt.hist(drive_data['ans'])\n",
    "#plt.show\n",
    "\n",
    "#Creating acceleration from velocity and time\n",
    "drive_data['velocity']=(drive_data['velocity']*5.0)/18.0\n",
    "drive_data['vdelta'] = (drive_data['velocity']-drive_data['velocity'].shift())\n",
    "drive_data['vdelta'][drive_data['row_count']==1]=0.0\n",
    "drive_data['accn'] = drive_data['vdelta'].astype(float) / drive_data['tdelta'].astype(float)\n",
    "#plt.plot(drive_data['vdelta'],drive_data['row_count'])\n",
    "\n",
    "drive_data['decel'] = 0\n",
    "drive_data['decel'][drive_data['vdelta']<0] = 1\n",
    "\n",
    "drive_data['acel'] = 0\n",
    "drive_data['acel'][drive_data['vdelta']>0] = 1\n",
    "\n",
    "drive_data['ft_sum_time_deaccel_val'] = drive_data['decel'] * drive_data['tdelta']\n",
    "drive_data['ft_sum_time_accel_val'] = drive_data['acel'] * drive_data['tdelta']\n",
    "\n",
    "drive_data['temp'] = drive_data['decel'].diff()\n",
    "drive_data['ft_cnt_vehicle_deaccel_val'] = drive_data['temp']\n",
    "drive_data['ft_cnt_vehicle_deaccel_val'][drive_data['temp']==-1] = 0\n",
    "drive_data['ft_cnt_vehicle_deaccel_val'][drive_data['row_count']==1] = 0\n",
    "\n",
    "drive_data['temp'] = drive_data['acel'].diff()\n",
    "drive_data['ft_cnt_vehicle_accel_val'] = drive_data['temp']\n",
    "drive_data['ft_cnt_vehicle_accel_val'][drive_data['temp']==-1] = 0\n",
    "drive_data['ft_cnt_vehicle_accel_val'][drive_data['row_count']==1] = 0\n",
    "\n",
    "#Calculating decel handbreak\n",
    "drive_data['decel_less_eg_10'] = 0\n",
    "drive_data['decel_less_eg_10'][drive_data['vdelta']<=-10.0] = 1\n",
    "\n",
    "drive_data['decel_eg_10'] = 0\n",
    "drive_data['decel_eg_10'][drive_data['vdelta']==-10.0] = 1\n",
    "\n",
    "drive_data['tag_decel'] = False\n",
    "mask = (drive_data.decel == 1)\n",
    "drive_data.loc[mask,'tag_decel'] = True\n",
    "\n",
    "drive_data_red = drive_data[['trip_id', 'vdelta', 'decel_less_eg_10', 'decel_eg_10', 'tag_decel']]\n",
    "\n",
    "trip_no = pd.unique(drive_data_red['trip_id'])\n",
    "\n",
    "decel_hb = pd.DataFrame()\n",
    "\n",
    "def decel_count(df):\n",
    "    return (\n",
    "        df.trip_id.iat[0],\n",
    "        df.vdelta.min(), \n",
    "        df.decel_less_eg_10.sum(), \n",
    "        df.decel_eg_10.sum(), \n",
    "        df.tag_decel.iat[0])\n",
    "    \n",
    "for i in range(0,len(trip_no)):\n",
    "    #print(i)\n",
    "    tn = trip_no[i]\n",
    "    sub = drive_data_red[drive_data_red['trip_id']==tn]\n",
    "    dcl = pd.DataFrame(zip(*sub.groupby(\n",
    "            (sub.tag_decel != sub.tag_decel.shift())\n",
    "            .cumsum()\n",
    "             ).apply(decel_count))).T\n",
    "    dcl.columns = ['trip_id', 'vdelta', 'decel_less_eg_10', 'decel_eg_10', 'tag_decel']\n",
    "    decel_hb = decel_hb.append(dcl)\n",
    "    \n",
    "decel_hb['vdelta'] = decel_hb['vdelta'].astype(float)\n",
    "decel_hb['decel_less_eg_10'] = decel_hb['decel_less_eg_10'].astype(int)\n",
    "decel_hb['decel_eg_10'] = decel_hb['decel_eg_10'].astype(int)\n",
    "decel_hb['tag_decel'] = decel_hb['tag_decel'].astype(bool)\n",
    "\n",
    "decel_hb['ft_sum_hard_brakes_10_flg_val'] = decel_hb['decel_less_eg_10'] - decel_hb['decel_eg_10']    \n",
    "decel_hb['ft_sum_hard_brakes_10_flg_val'][decel_hb['decel_less_eg_10'] == decel_hb['decel_eg_10']] = decel_hb['decel_less_eg_10']\n",
    "decel_hb['ft_sum_hard_brakes_3_flg_val'] = 0\n",
    "decel_hb['ft_sum_hard_brakes_3_flg_val'][(decel_hb['vdelta'] <= -3.0) & (decel_hb['vdelta'] > -10.0)] = 1\n",
    "decel_hb_summ = decel_hb.groupby(['trip_id'], as_index=False)['ft_sum_hard_brakes_10_flg_val','ft_sum_hard_brakes_3_flg_val'].sum()\n",
    "del drive_data_red\n",
    "\n",
    "#Calculating accel handbreak\n",
    "drive_data['acel_gt_eg_10'] = 0\n",
    "drive_data['acel_gt_eg_10'][drive_data['vdelta']>=10.0] = 1\n",
    "\n",
    "drive_data['acel_eg_10'] = 0\n",
    "drive_data['acel_eg_10'][drive_data['vdelta'] == 10.0] = 1\n",
    "\n",
    "drive_data['tag_acel'] = False\n",
    "mask = (drive_data.acel == 1)\n",
    "drive_data.loc[mask,'tag_acel'] = True\n",
    "\n",
    "drive_data_red = drive_data[['trip_id', 'vdelta', 'acel_gt_eg_10', 'acel_eg_10', 'tag_acel']]\n",
    "acel_hb = pd.DataFrame()\n",
    "\n",
    "def acel_count(df):\n",
    "    return (\n",
    "        df.trip_id.iat[0],\n",
    "        df.vdelta.max(), \n",
    "        df.acel_gt_eg_10.sum(), \n",
    "        df.acel_eg_10.sum(), \n",
    "        df.tag_acel.iat[0])\n",
    "\n",
    "for i in range(0,len(trip_no)):\n",
    "    #print(i)\n",
    "    tn = trip_no[i]\n",
    "    sub = drive_data_red[drive_data_red['trip_id']==tn]\n",
    "    acl = pd.DataFrame(zip(*sub.groupby(\n",
    "            (sub.tag_acel != sub.tag_acel.shift())\n",
    "            .cumsum()\n",
    "             ).apply(acel_count))).T\n",
    "    acl.columns = ['trip_id', 'vdelta', 'acel_gt_eg_10', 'acel_eg_10', 'tag_acel']\n",
    "    acel_hb = acel_hb.append(acl)\n",
    "\n",
    "acel_hb['vdelta'] = acel_hb['vdelta'].astype(float)\n",
    "acel_hb['acel_gt_eg_10'] = acel_hb['acel_gt_eg_10'].astype(int)\n",
    "acel_hb['acel_eg_10'] = acel_hb['acel_eg_10'].astype(int)\n",
    "acel_hb['tag_decel'] = acel_hb['tag_acel'].astype(bool)\n",
    "\n",
    "acel_hb['ft_sum_hard_brakes_10_flg_val'] = acel_hb['acel_gt_eg_10'] - acel_hb['acel_eg_10']    \n",
    "acel_hb['ft_sum_hard_brakes_10_flg_val'][acel_hb['acel_gt_eg_10'] == acel_hb['acel_eg_10']] = acel_hb['acel_gt_eg_10']\n",
    "acel_hb['ft_sum_hard_brakes_3_flg_val'] = 0\n",
    "acel_hb['ft_sum_hard_brakes_3_flg_val'][(acel_hb['vdelta'] >= 3.0) & (acel_hb['vdelta'] < 10.0)] = 1\n",
    "\n",
    "acel_hb_summ = acel_hb.groupby(['trip_id'], as_index=False)['ft_sum_hard_brakes_10_flg_val','ft_sum_hard_brakes_3_flg_val'].sum()\n",
    "acel_hb_summ=acel_hb_summ.rename(columns = {'ft_sum_hard_brakes_10_flg_val':'ft_sum_hard_accel_10_flg_val'})\n",
    "acel_hb_summ=acel_hb_summ.rename(columns = {'ft_sum_hard_brakes_3_flg_val':'ft_sum_hard_accel_3_flg_val'})\n",
    "\n",
    "acel_dcel = pd.merge(decel_hb_summ, acel_hb_summ, how='left', on=['trip_id'])\n",
    "\n",
    "drive_data_red =  drive_data[['trip_id', 'ft_sum_time_deaccel_val', 'ft_sum_time_accel_val', 'ft_cnt_vehicle_deaccel_val', 'ft_cnt_vehicle_accel_val']]\n",
    "drive_data_red_summ = drive_data_red.groupby(['trip_id'], as_index=False).sum()\n",
    "\n",
    "drive_features = pd.merge(acel_dcel, drive_data_red_summ, how='left', on=['trip_id'])\n",
    "\n",
    "del acel_dcel, acel_hb, acel_hb_summ, acl, dcl, mask\n",
    "del decel_hb, decel_hb_summ, drive_data_red, sub, trip_no\n",
    "del drive_data_red_summ, tn, vehicle_data\n",
    "drive_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_data = drive_data.drop(columns=['ft_sum_time_deaccel_val','ft_sum_time_accel_val','temp','ft_cnt_vehicle_deaccel_val'])\n",
    "drive_data = drive_data.drop(columns=['ft_cnt_vehicle_accel_val','decel_less_eg_10','decel_eg_10','tag_decel','acel_gt_eg_10'])\n",
    "drive_data = drive_data.drop(columns=['acel_eg_10','tag_acel','decel','acel'])\n",
    "del trip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['datetime'] = weather_data['date'].astype(str) + ' ' + weather_data['time'].astype(str)\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'],format = '%Y-%m-%d %H' ,utc = False)\n",
    "weather_data['datetime'] = weather_data['datetime'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "weather_data['Temp_F'] = (weather_data['temperature_data'].astype(float) - 273.0)*1.8 + 32.0\n",
    "\n",
    "weather_data['w_cond'] = 'SNOW'\n",
    "weather_data['w_cond'][(weather_data['temperature_data']> 27.0) & (weather_data['temperature_data']<= 32.0) ] = 'FREEZING RAIN'\n",
    "weather_data['w_cond'][weather_data['temperature_data']> 32.0 ] = 'RAIN'\n",
    "\n",
    "weather_data['w_perc'] = 'LIGHT'\n",
    "weather_data['w_perc'][(weather_data['precipitation_data']> 2.5) & (weather_data['precipitation_data']<= 7.6) ] = 'MODERATE'\n",
    "weather_data['w_perc'][weather_data['precipitation_data']> 7.6 ] = 'HEAVY'\n",
    "\n",
    "weather_data['geohash'] = weather_data.apply(lambda x: gh.encode(x.lat, x.lon, precision = 5),axis = 1)\n",
    "\n",
    "#Weather features calculations\n",
    "weather_data['datetime'] = weather_data['date'].astype(str) + ' ' + weather_data['time'].astype(str)\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'],format = '%Y-%m-%d %H' ,utc = False)\n",
    "weather_data['datetime'] = weather_data['datetime'].dt.tz_convert('US/Pacific')\n",
    "weather_data['geohash'] = weather_data.apply(lambda x: gh.encode(x.lat, x.lon, precision = 5),axis = 1)\n",
    "\n",
    "weather_data['Temp_F'] = (weather_data['temperature_data'].astype(float) - 273.0)*1.8 + 32.0\n",
    "\n",
    "weather_data['w_cond'] = 'SNOW'\n",
    "weather_data['w_cond'][(weather_data['temperature_data']> 27.0) & (weather_data['temperature_data']<= 32.0) ] = 'FREEZING RAIN'\n",
    "weather_data['w_cond'][weather_data['temperature_data']> 32.0 ] = 'RAIN'\n",
    "\n",
    "weather_data['w_perc'] = 'LIGHT'\n",
    "weather_data['w_perc'][(weather_data['precipitation_data']> 2.5) & (weather_data['precipitation_data']<= 7.6) ] = 'MODERATE'\n",
    "weather_data['w_perc'][weather_data['precipitation_data']> 7.6 ] = 'HEAVY'\n",
    "\n",
    "weather_data = weather_data.drop(columns=['wind_ew_data','wind_ew_unit','wind_ns_data','wind_ns_unit'])\n",
    "\n",
    "weather_red = weather_data[['date','time','datetime','geohash','Temp_F','w_cond','w_perc']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive data summary & Harvesine formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_data_red = drive_data[['vehicle_id','datetime','lat','long','geohash','velocity','First_day_of_the_week','row_count']]\n",
    "drive_data_red['a'] = pd.to_datetime(drive_data_red['datetime'].dt.strftime('%Y-%m-%d %H'))\n",
    "drive_data_red = drive_data_red.drop(columns=['datetime'])\n",
    "drive_data_red=drive_data_red.rename(columns = {'a':'datetime'})\n",
    "\n",
    "drive_data_red.reset_index(level=0, inplace=True)\n",
    "drive_data_red = drive_data_red.drop(columns=['index'])\n",
    "drive_data_red['a'] = drive_data_red['datetime'].dt.tz_localize('US/Pacific')\n",
    "drive_data_red = drive_data_red.drop(columns=['datetime'])\n",
    "drive_data_red=drive_data_red.rename(columns = {'a':'datetime'})\n",
    "\n",
    "drive_data_red['lat2'] = drive_data_red['lat'].shift()\n",
    "drive_data_red['long2'] = drive_data_red['long'].shift()\n",
    "drive_data_red['lat2'][drive_data_red['row_count']==1] = 0.0\n",
    "drive_data_red['long2'][drive_data_red['row_count']==1] = 0\n",
    "\n",
    "lat_1 = np.array(drive_data_red['lat'])\n",
    "lat_2 = np.array(drive_data_red['lat2'])\n",
    "\n",
    "long_1 = np.array(drive_data_red['long'])\n",
    "long_2 = np.array(drive_data_red['long2'])\n",
    "\n",
    "hv = pd.Series(np.zeros(len(drive_data_red)).astype(float))\n",
    "\n",
    "for i in range(0,len(drive_data_red)):\n",
    "    hv[i] = haversine((lat_1[i],long_1[i]),(lat_2[i],long_2[i]),unit=Unit.KILOMETERS)\n",
    "    \n",
    "drive_data_red['hv'] = hv\n",
    "drive_data_red['hv'][drive_data_red['row_count']==1] = 0\n",
    "\n",
    "del lat_1, lat_2, long_1, long_2, hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Feature Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>First_day_of_the_week</th>\n",
       "      <th>total_light_rain_driving_km</th>\n",
       "      <th>total_light_freezing_rain_driving_km</th>\n",
       "      <th>total_light_snow_driving_km</th>\n",
       "      <th>total_moderate_rain_driving_km</th>\n",
       "      <th>total_moderate_freezing_rain_driving_km</th>\n",
       "      <th>total_moderate_snow_driving_km</th>\n",
       "      <th>total_heavy_snow_driving_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>35.176023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1841.102752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.788228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>164.381380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>463.731471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.045343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000500</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>169.205173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id First_day_of_the_week  total_light_rain_driving_km  \\\n",
       "0     1000500            2016-12-26                    35.176023   \n",
       "1     1000500            2017-01-02                  1841.102752   \n",
       "2     1000500            2017-01-09                   164.381380   \n",
       "3     1000500            2017-01-16                   463.731471   \n",
       "4     1000500            2017-01-23                   169.205173   \n",
       "\n",
       "   total_light_freezing_rain_driving_km  total_light_snow_driving_km  \\\n",
       "0                                   0.0                          0.0   \n",
       "1                                   0.0                          0.0   \n",
       "2                                   0.0                          0.0   \n",
       "3                                   0.0                          0.0   \n",
       "4                                   0.0                          0.0   \n",
       "\n",
       "   total_moderate_rain_driving_km  total_moderate_freezing_rain_driving_km  \\\n",
       "0                        0.000000                                      0.0   \n",
       "1                       50.788228                                      0.0   \n",
       "2                        0.000000                                      0.0   \n",
       "3                       60.045343                                      0.0   \n",
       "4                        0.000000                                      0.0   \n",
       "\n",
       "   total_moderate_snow_driving_km  total_heavy_snow_driving_km  \n",
       "0                             0.0                          0.0  \n",
       "1                             0.0                          0.0  \n",
       "2                             0.0                          0.0  \n",
       "3                             0.0                          0.0  \n",
       "4                             0.0                          0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_data_merge = pd.merge(drive_data_red, weather_red, how='left', on=['datetime','geohash'])\n",
    "drive_data_merge.isnull().sum(axis = 0)\n",
    "\n",
    "drive_data_merge['total_light_rain_driving_km'] = 0.0\n",
    "drive_data_merge['total_light_rain_driving_km'][(drive_data_merge['w_cond']=='RAIN') & (drive_data_merge['w_perc']=='LIGHT')] = drive_data_merge['hv']\n",
    " \n",
    "drive_data_merge['total_light_freezing_rain_driving_km'] = 0.0\n",
    "drive_data_merge['total_light_freezing_rain_driving_km'][(drive_data_merge['w_cond']=='FREEZING RAIN') & (drive_data_merge['w_perc']=='LIGHT')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge['total_light_snow_driving_km'] = 0.0\n",
    "drive_data_merge['total_light_snow_driving_km'][(drive_data_merge['w_cond']=='SNOW') & (drive_data_merge['w_perc']=='LIGHT')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge['total_moderate_rain_driving_km'] = 0.0\n",
    "drive_data_merge['total_moderate_rain_driving_km'][(drive_data_merge['w_cond']=='RAIN') & (drive_data_merge['w_perc']=='MODERATE')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge['total_moderate_freezing_rain_driving_km'] = 0.0\n",
    "drive_data_merge['total_moderate_freezing_rain_driving_km'][(drive_data_merge['w_cond']=='FREEZING RAIN') & (drive_data_merge['w_perc']=='MODERATE')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge['total_moderate_snow_driving_km'] = 0.0\n",
    "drive_data_merge['total_moderate_snow_driving_km'][(drive_data_merge['w_cond']=='SNOW') & (drive_data_merge['w_perc']=='MODERATE')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge['total_heavy_snow_driving_km'] = 0.0\n",
    "drive_data_merge['total_heavy_snow_driving_km'][(drive_data_merge['w_cond']=='SNOW') & (drive_data_merge['w_perc']=='HEAVY')] = drive_data_merge['hv']\n",
    "\n",
    "drive_data_merge_sub = drive_data_merge[['vehicle_id','First_day_of_the_week','total_light_rain_driving_km','total_light_freezing_rain_driving_km','total_light_snow_driving_km','total_moderate_rain_driving_km','total_moderate_freezing_rain_driving_km','total_moderate_snow_driving_km','total_heavy_snow_driving_km']]\n",
    "weather_features = drive_data_merge_sub.groupby(['vehicle_id','First_day_of_the_week'], as_index=False).sum()\n",
    "\n",
    "del drive_data_merge_sub, drive_data_merge\n",
    "weather_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting unwanted dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather_data, weather_red"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
